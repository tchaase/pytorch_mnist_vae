{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Variational Autoencoder for the MNIST-Data-Set\n",
    "\n",
    "Tobias Haase\n",
    "\n",
    "I am slightly orienting myself on a paper from [Abid & Zou (2019)](https://arxiv.org/abs/1902.04601)\n",
    "## Set Up\n",
    "Firstly I am loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import imageio \n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/tchaase/Documents/Universitaet/Master-Arbeit/pytorch_mnist_VAE/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the [vae](vae.ipynb) I will firstly also define the utility functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, z_mu, z_logvar, s_mu=None, s_logvar=None):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: reconstruction loss\n",
    "    :param z_mu: mean from the latent vector of encoder_z\n",
    "    :param z_logvar: log variance from the latent vector of encoder_z\n",
    "    :param s_mu: mean from the latent vector of encoder_s (optional)\n",
    "    :param s_logvar: log variance from the latent vector of encoder_s (optional)\n",
    "    \"\"\"\n",
    "    BCE = bce_loss\n",
    "    KLD_z = -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp())\n",
    "    if s_mu is not None and s_logvar is not None:\n",
    "        KLD_s = -0.5 * torch.sum(1 + s_logvar - s_mu.pow(2) - s_logvar.exp())\n",
    "        return BCE + KLD_z + KLD_s\n",
    "    else:\n",
    "        return BCE + KLD_z\n",
    "    \n",
    "    # cross entropy - guck dir die LossFunktion an. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i, (data, is_overlayed) in tqdm(enumerate(dataloader), total=len(dataset)):\n",
    "        data = data.to(device)\n",
    "        overlay_status = is_overlayed.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z_mean, z_log_var, s_mean, s_log_var, reconstructed_data = model(data, overlay_status)\n",
    "        bce_loss = criterion(reconstructed_data, data)\n",
    "        loss = final_loss(bce_loss, z_mean, z_log_var, s_mean, s_log_var)\n",
    "        loss.backward()  # Using the loss, backpropagation occurs. Thus, all the tensors that will be connected to this, will be involved in this computation. \n",
    "        running_loss += loss.item() #This here defined for every step along the way, how high is the loss. \n",
    "    \n",
    "        optimizer.step()\n",
    "        counter += data.size(0)\n",
    "    train_loss = running_loss / counter\n",
    "    return train_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the validation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "            counter += 1\n",
    "            data = data[0]\n",
    "            data = data.to(device)\n",
    "            reconstruction, z_mu, z_logvar, s_mu, s_logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, z_mu, z_logvar, s_mu, s_logvar)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
    "                recon_images = reconstruction\n",
    "    val_loss = running_loss / counter\n",
    "    return val_loss, recon_images\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lastly I will define some parameters that I will use before I define the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4 \n",
    "init_channels = 8 # initial number of filters, first layers output. \n",
    "image_channels = 1 # MNIST images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling\n",
    "stride = 2\n",
    "channels = 1  #working with grayscale. \n",
    "lr = 0.001\n",
    "same = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "latent_dim = 16\n",
    "\n",
    "\n",
    "class EncoderZ(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.fc1 = nn.Linear(init_channels*2* 8 * 8, 256) # t applies a linear transformation to the input data, which means it computes the dot product of the input and weight tensors, adds the bias term, and produces the output.\n",
    "        self.fc_mean = nn.Linear(256, (16))\n",
    "        self.fc_log_var = nn.Linear(256, (16))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(batch_size,-1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        z_mean = self.fc_mean(h)  \n",
    "        z_log_var = self.fc_log_var(h)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class EncoderS(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(EncoderS, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.fc1 = nn.Linear(init_channels * 2*8 * 8, 256)\n",
    "        self.fc_mean = nn.Linear(256, (16))\n",
    "        self.fc_log_var = nn.Linear(256, (16))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(batch_size_target,-1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        s_mean = self.fc_mean(h)\n",
    "        s_log_var = self.fc_log_var(h)\n",
    "        return s_mean, s_log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear((16* 2), 256)\n",
    "        self.fc2 = nn.Linear(256, 2048)  # Adjust the output size of fc2\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=init_channels*4, out_channels=init_channels*2, kernel_size=4, stride=2, padding=1)  # Set input channels to 1\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=init_channels*2, out_channels = image_channels, kernel_size=4, stride=2, padding= 1 )\n",
    "\n",
    "    def forward(self, zs):\n",
    "        x = F.relu(self.fc1(zs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(batch_size, 32, 8, 8)  # Reshape to a 4-dimensional ten\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class cVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(cVAE, self).__init__()\n",
    "        self.encoder_z = EncoderZ(latent_dim)\n",
    "        self.encoder_s = EncoderS(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.overlay_status = None \n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + epsilon * std\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, target_image, bg_image):\n",
    "        z_mean, z_log_var = self.encoder_z(torch.cat([target_images, bg_images], dim=0))  # This way, the encoder z should be trained using 128 batches of data, with and without the target. \n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        \n",
    "        torch.zeros_like\n",
    "        s_mean, s_log_var = self.encoder_s(target_images)\n",
    "        s = self.reparameterize(s_mean, s_log_var)      \n",
    "\n",
    "        empty_vector = torch.zeros_like(s).detach()\n",
    "        s_with_empty_vector = torch.cat([s,empty_vector], dim = 0)\n",
    "        \n",
    "\n",
    "        zs = torch.cat([z, s_with_empty_vector], dim=1)\n",
    "\n",
    "        reconstructed_data = self.decoder(zs)\n",
    "        \n",
    "# Still playing around with this. If I detach a tensor and then cat them, the resulting tensor will not have half a tensor thats detached. I need to figure out how I can deactivate the second encoder...\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, I can't directly access the images to superimpose them. Therefore, I will have to get the images with the following function. Additionally, I have incorporated a greyscaler into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def load_databatch(data_folder, idx, img_size=32):\n",
    "    data_file = os.path.join(data_folder, 'train_data_batch_' + str(idx))\n",
    "    d = unpickle(data_file)\n",
    "    x = d['data']\n",
    "    y = d['labels']\n",
    "    mean_image = d['mean']\n",
    "\n",
    "    x = x / np.float32(255)\n",
    "    mean_image = mean_image / np.float32(255)\n",
    "\n",
    "    # Labels are indexed from 1, shift it so that indexes start at 0\n",
    "    y = [i - 1 for i in y]\n",
    "    data_size = x.shape[0]\n",
    "\n",
    "    x -= mean_image\n",
    "\n",
    "    img_size2 = img_size * img_size\n",
    "\n",
    "    x = np.dstack((x[:, :img_size2], x[:, img_size2:2 * img_size2], x[:, 2 * img_size2:]))\n",
    "    x = x.reshape((x.shape[0], img_size, img_size, 3)).transpose(0, 3, 1, 2)\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:data_size, :, :, :]\n",
    "    Y_train = y[0:data_size]\n",
    "    X_train_flip = X_train[:, :, :, ::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train, X_train_flip), axis=0)\n",
    "    Y_train = np.concatenate((Y_train, Y_train_flip), axis=0)\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train.astype(np.float32),\n",
    "        Y_train=np.array(Y_train, dtype=np.int32),\n",
    "        mean=mean_image.astype(np.float32)\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's superimpose them with each other!\n",
    "\n",
    "Firstly here is the way I loaded the mnist pictures as it was done before [here](./vae.ipynb). The data is loaded and resized to 32x32 to fit the Image-Net Data. \n",
    "I have once again split the data into a training and validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32, 32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=True, download=True, transform=transform\n",
    ")\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I am unpickleing one batch and doing the calculations that were previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = load_databatch(\"./input/Imagenet32_train\", 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets split the channels into matrices to then greyscale them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape is now:  (256232, 3, 32, 32)\n",
      "The shape is now:  (256232, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "image_data = batch_1[\"X_train\"]\n",
    "\n",
    "# Get the number of images\n",
    "num_images = len(image_data)\n",
    "\n",
    "# Initialize an empty array to store the reshaped grayscale images\n",
    "grayscale_images = np.empty((num_images, 32, 32), dtype=np.float32)\n",
    "\n",
    "print(\"The shape is now: \",image_data.shape)\n",
    "\n",
    "# Reshape the images\n",
    "for i, image in enumerate(image_data):\n",
    "    # Reshape the image to match the color channel dimensions (3x32x32)\n",
    "    image = np.reshape(image, (3, 32, 32))\n",
    "\n",
    "    # Split the image into separate color channels\n",
    "    red_channel = image[0]\n",
    "    green_channel = image[1]\n",
    "    blue_channel = image[2]\n",
    "\n",
    "    # Combine the color channels weighted by their respective coefficients to form the grayscale image - these values are taken from recommedations from ChatGPT\n",
    "    grayscale_image = 0.2989 * red_channel + 0.5870 * green_channel + 0.1140 * blue_channel\n",
    "\n",
    "    # Normalize the grayscale image to the range [0, 1]\n",
    "    grayscale_image /= 255.0\n",
    "\n",
    "    # Store the grayscale image in the array\n",
    "    grayscale_images[i] = grayscale_image\n",
    "\n",
    "# Store the reshaped grayscale images in a new entry in the dictionary\n",
    "batch_1[\"X_train_gray\"] = grayscale_images\n",
    "print(\"The shape is now: \",grayscale_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seemed to have worked. Now, lets move on to overlaying them. The following still needs proper testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlayed Images: 30096\n",
      "Not Overlayed Images: 226136\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "mnist_data = datasets.MNIST(root='./input', train=True, download=True, transform=transform)\n",
    "\n",
    "# Access the background images from the dictionary\n",
    "background_images = batch_1[\"X_train_gray\"]\n",
    "\n",
    "# Create a list to store the overlaid images\n",
    "target_images = []\n",
    "overlayed_count = 0\n",
    "\n",
    "# Create a list to store the background images (and load them)\n",
    "bg_images = []\n",
    "not_overlayed_count = 0\n",
    "\n",
    "# Assuming background_images and mnist_data are lists\n",
    "\n",
    "for i in range(len(background_images)):\n",
    "    background_image = background_images[i]\n",
    "    random_overlay = random.random() < 0.5  # Randomly choose whether to overlay MNIST image or not\n",
    "\n",
    "    if random_overlay and i < len(mnist_data):  # Overlay MNIST image\n",
    "        random_index = random.randint(0, len(mnist_data) - 1)  # Choose a random index from mnist_data\n",
    "        mnist_image, _ = mnist_data[random_index]\n",
    "\n",
    "        # Convert background image to tensor and move it to the CUDA device\n",
    "        background_tensor = torch.tensor(background_image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Resize the background image to match the MNIST image dimensions\n",
    "        resized_background_tensor = transforms.Resize((32, 32))(background_tensor)\n",
    "\n",
    "        # Resize MNIST image to match the background image dimensions\n",
    "        mnist_resized = transforms.Resize((32, 32))(mnist_image)\n",
    "\n",
    "        overlaid_tensor = resized_background_tensor + mnist_resized.to(device)\n",
    "\n",
    "        target_images.append(overlaid_tensor)  # Store information that the image was overlaid\n",
    "        overlayed_count += 1\n",
    "\n",
    "    else:\n",
    "        background_tensor = torch.tensor(background_image).unsqueeze(0).to(device)\n",
    "        bg_images.append(background_tensor)  # Store information that the image was not overlaid\n",
    "        not_overlayed_count += 1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# Print the counts of overlaid and non-overlaid images\n",
    "print(f\"Overlayed Images: {overlayed_count}\")\n",
    "print(f\"Not Overlayed Images: {not_overlayed_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I was a not so smart person before and started concatenating the images instead of using element wise addition, here is a constant reminder that I have now fixed this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of background images: 226136\n",
      "Number of overlaid images: 30096\n",
      "Background image dimensions: torch.Size([1, 32, 32])\n",
      "Overlaid image dimensions: torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "def check_image_dimensions(bg_images, target_images):\n",
    "    background_count = len(bg_images)\n",
    "    overlaid_count = len(target_images)\n",
    "\n",
    "    if background_count > 0:\n",
    "        background_dim = bg_images[0].shape\n",
    "    else:\n",
    "        background_dim = None\n",
    "\n",
    "    if overlaid_count > 0:\n",
    "        overlaid_dim = target_images[0].shape\n",
    "    else:\n",
    "        overlaid_dim = None\n",
    "\n",
    "    return background_count, overlaid_count, background_dim, overlaid_dim\n",
    "\n",
    "# Usage example:\n",
    "background_count, overlaid_count, background_dim, overlaid_dim = check_image_dimensions(bg_images, target_images)\n",
    "\n",
    "print(f\"Number of background images: {background_count}\")\n",
    "print(f\"Number of overlaid images: {overlaid_count}\")\n",
    "print(f\"Background image dimensions: {background_dim}\")\n",
    "print(f\"Overlaid image dimensions: {overlaid_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I check if anything is not a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_images:\n",
    "    if not torch.is_tensor(target) or target.shape != (1, 32, 32):\n",
    "        print(\"Non-tensor overlaid image found!\")\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for background in bg_images:\n",
    "    if not torch.is_tensor(background):\n",
    "        print(\"Non-tensor background found!\")\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the data-loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class OverlaidDataset(Dataset):\n",
    "    def __init__(self, target_images, bg_images):\n",
    "        self.target_images = target_images\n",
    "        self.bg_images = bg_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bg_images)  # Use the length of the background images list as the dataset length. I got more background images. \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_image = self.target_images[index % len(self.target_images)]  # Cycle through target images\n",
    "        bg_image = self.bg_images[index]\n",
    "\n",
    "        return target_image, bg_image\n",
    "\n",
    "# Create the overlaid dataset\n",
    "overlaid_dataset = OverlaidDataset(target_images, bg_images)\n",
    "\n",
    "# Create the dataloader\n",
    "batch_size = 64\n",
    "shuffle = True  # Shuffle the dataset to mix target and background images\n",
    "overlaid_dataloader = DataLoader(overlaid_dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking again that I still have the right data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_image is not a tensor of size 32x32. Instead it is:  torch.Size([24, 1, 32, 32])\n",
      "bg_image is not a tensor of size 32x32. Instead it is:  torch.Size([24, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for target_image, bg_image in overlaid_dataloader:\n",
    "    # Check if target_image is a tensor of size 32x32\n",
    "    if isinstance(target_image, torch.Tensor) and target_image.shape == (64, 1, 32, 32):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"target_image is not a tensor of size 32x32. Instead it is: \", target_image.shape)\n",
    "\n",
    "    # Check if bg_image is a tensor of size 32x32\n",
    "    if isinstance(bg_image, torch.Tensor) and bg_image.shape == (64, 1, 32, 32):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"bg_image is not a tensor of size 32x32. Instead it is: \", bg_image.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am creating a training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256232 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      7\u001b[0m     \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, overlaid_dataloader, overlaid_dataset, device, optimizer, criterion)\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Validate the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     val_loss, recon_images \u001b[39m=\u001b[39m validate(model, overlaid_dataloader, overlaid_dataset, device, criterion)\n",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, dataset, device, optimizer, criterion)\u001b[0m\n\u001b[1;32m      7\u001b[0m overlay_status \u001b[39m=\u001b[39m is_overlayed\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m z_mean, z_log_var, s_mean, s_log_var, reconstructed_data \u001b[39m=\u001b[39m model(data, overlay_status)\n\u001b[1;32m     12\u001b[0m bce_loss \u001b[39m=\u001b[39m criterion(reconstructed_data, data)\n\u001b[1;32m     13\u001b[0m loss \u001b[39m=\u001b[39m final_loss(bce_loss, z_mean, z_log_var, s_mean, s_log_var)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[34], line 79\u001b[0m, in \u001b[0;36mcVAE.forward\u001b[0;34m(self, x, overlay)\u001b[0m\n\u001b[1;32m     74\u001b[0m z_mean, z_log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_z(x)\n\u001b[1;32m     75\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(z_mean, z_log_var)\n\u001b[0;32m---> 79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverlay_status: \n\u001b[1;32m     80\u001b[0m     mean, s_log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_s(x)\n\u001b[1;32m     81\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(s_mean, s_log_var)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "model = cVAE(latent_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    train_loss = train(model, overlaid_dataloader, overlaid_dataset, device, optimizer, criterion)\n",
    "    \n",
    "    # Validate the model\n",
    "    val_loss, recon_images = validate(model, overlaid_dataloader, overlaid_dataset, device, criterion)\n",
    "    \n",
    "    # Print the losses\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
