{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Variational Autoencoder for the MNIST-Data-Set\n",
    "\n",
    "Tobias Haase\n",
    "\n",
    "I am slightly orienting myself on a paper from [Abid & Zou (2019)](https://arxiv.org/abs/1902.04601)\n",
    "## Set Up\n",
    "Firstly I am loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import imageio \n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/tchaase/Documents/Universitaet/Master-Arbeit/pytorch_mnist_VAE/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the [vae](vae.ipynb) I will firstly also define the utility functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, z_mu, z_logvar, s_mu, s_logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: reconstruction loss\n",
    "    :param z_mu: mean from the latent vector of encoder_z\n",
    "    :param z_logvar: log variance from the latent vector of encoder_z\n",
    "    :param s_mu: mean from the latent vector of encoder_s (optional)\n",
    "    :param s_logvar: log variance from the latent vector of encoder_s (optional)\n",
    "    \"\"\"\n",
    "    BCE = bce_loss\n",
    "    KLD_z = -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp())\n",
    "    if s_mu is not None and s_logvar is not None:\n",
    "        KLD_s = -0.5 * torch.sum(1 + s_logvar - s_mu.pow(2) - s_logvar.exp())\n",
    "        return BCE + KLD_z + KLD_s\n",
    "    else:\n",
    "        return BCE + KLD_z\n",
    "    \n",
    "    # Use cross entropy for mri images perhaps, more appropriate than bce. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss_bg = 0.0\n",
    "    running_loss_target = 0.0\n",
    "    counter = 0\n",
    "\n",
    "    total_batches = len(dataset) // dataloader.batch_size\n",
    "\n",
    "    for i, (target_data, background_data) in tqdm(enumerate(dataloader), total= total_batches):\n",
    "        target_data = target_data.to(device)\n",
    "        background_data = background_data.to(device)\n",
    "                       \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z_mean, z_log_var, s_mean, s_log_var, z_mean_bg, z_log_var_bg, reconstructed_data_target, reconstructed_data_bg = model(target_data, background_data)\n",
    "\n",
    "        # Section for the target images\n",
    "        bce_loss_target = criterion(reconstructed_data_target, target_data)\n",
    "        loss_target = final_loss(bce_loss_target, z_mean, z_log_var, s_mean, s_log_var)\n",
    "        running_loss_target += loss_target.item()\n",
    "        loss_target.backward()\n",
    "\n",
    "        # Section for the background images\n",
    "        bce_loss_background = criterion(reconstructed_data_bg, background_data)\n",
    "        s_mean_bg, s_log_var_bg = None, None\n",
    "        loss_background = final_loss(bce_loss_background, z_mean_bg, z_log_var_bg, s_mean_bg, s_log_var_bg)\n",
    "        loss_background.backward()  # Using the loss, backpropagation occurs. Thus, all the tensors that will be connected to this, will be involved in this computation. \n",
    "        running_loss_bg += loss_background.item() #This here defined for every step along the way, how high is the loss. \n",
    "    \n",
    "        optimizer.step()\n",
    "        counter += len(target_data)\n",
    "    train_loss_target = running_loss_target / counter\n",
    "    train_loss_bg = running_loss_bg / counter\n",
    "    return train_loss_target, train_loss_bg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the validation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    \n",
    "    total_batches = len(dataset) // dataloader.batch_size\n",
    "    with torch.no_grad():\n",
    "        for i, (target_data, background_data) in tqdm(enumerate(dataloader), total=total_batches):\n",
    "            background_data = background_data.to(device)\n",
    "            \n",
    "            # Perform the forward pass\n",
    "            z_mean, z_log_var, s_mean, s_log_var, z_mean_bg, z_log_var_bg, reconstructed_data_target, reconstructed_data_bg = model(target_data, background_data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            bce_loss_target = criterion(reconstructed_data_target, target_data)\n",
    "            loss_target = final_loss(bce_loss_target, z_mean, z_log_var, s_mean, s_log_var)\n",
    "            running_loss += loss_target.item()\n",
    "            \n",
    "            counter += target_data.size(0)  # Increment the counter by the batch size (number of samples in the current batch)\n",
    "            \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == len(dataloader) - 1:\n",
    "                recon_images = reconstructed_data_target\n",
    "    \n",
    "    val_loss = running_loss / counter\n",
    "    return val_loss, recon_images\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lastly I will define some parameters that I will use before I define the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4 \n",
    "init_channels = 8 # initial number of filters, first layers output. \n",
    "image_channels = 1 # MNIST images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling\n",
    "stride = 2\n",
    "channels = 1  #working with grayscale. \n",
    "lr = 0.001\n",
    "same = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.randn(64, 16, 8, 8)\n",
    "h = tensor.view(64,-1)\n",
    "\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "latent_dim = 16\n",
    "\n",
    "\n",
    "class EncoderZ(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.fc1 = nn.Linear(init_channels*2* 8 * 8, 256) # t applies a linear transformation to the input data, which means it computes the dot product of the input and weight tensors, adds the bias term, and produces the output.\n",
    "        self.fc_mean = nn.Linear(256, (16))\n",
    "        self.fc_log_var = nn.Linear(256, (16))\n",
    "\n",
    "    def forward(self, x, batch_size):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(batch_size,-1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        z_mean = self.fc_mean(h)  \n",
    "        z_log_var = self.fc_log_var(h)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class EncoderS(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(EncoderS, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.fc1 = nn.Linear(init_channels * 2*8 * 8, 256)\n",
    "        self.fc_mean = nn.Linear(256, (16))\n",
    "        self.fc_log_var = nn.Linear(256, (16))\n",
    "\n",
    "    def forward(self, x, batch_size):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(batch_size,-1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        s_mean = self.fc_mean(h)\n",
    "        s_log_var = self.fc_log_var(h)\n",
    "        return s_mean, s_log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear((16* 2), 256)\n",
    "        self.fc2 = nn.Linear(256, 2048)  # Adjust the output size of fc2\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=init_channels*4, out_channels=init_channels*2, kernel_size=4, stride=2, padding=1)  # Set input channels to 1\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=init_channels*2, out_channels = image_channels, kernel_size=4, stride=2, padding= 1 )\n",
    "\n",
    "    def forward(self, zs, batch_size):\n",
    "        x = F.relu(self.fc1(zs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(batch_size, 32, 8, 8)  # Reshape to a 4-dimensional ten\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class cVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(cVAE, self).__init__()\n",
    "        self.encoder_z = EncoderZ(latent_dim)\n",
    "        self.encoder_s = EncoderS(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.overlay_status = None \n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + epsilon * std\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, target_image, bg_image):\n",
    "        batch_size = target_image.size(0) \n",
    "        z_mean, z_log_var = self.encoder_z(target_image, batch_size)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        s_mean, s_log_var = self.encoder_s(target_image, batch_size)\n",
    "        s = self.reparameterize(s_mean, s_log_var)      \n",
    "        zs = torch.cat([z, s], dim=1)\n",
    "\n",
    "        reconstructed_data_target = self.decoder(zs, batch_size)\n",
    "\n",
    "        z_mean_bg, z_log_var_bg = self.encoder_z(bg_image, batch_size)\n",
    "        z_bg = self.reparameterize(z_mean_bg, z_log_var_bg)\n",
    "        z_empty = torch.zeros_like(z_bg)\n",
    "        z_bg_0 = torch.cat([z_bg, z_empty], dim =1)\n",
    "        reconstructed_data_bg = self.decoder(z_bg_0, batch_size)\n",
    "        \n",
    "        return z_mean, z_log_var, s_mean, s_log_var, z_mean_bg, z_log_var_bg, reconstructed_data_target, reconstructed_data_bg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, I can't directly access the images to superimpose them. Therefore, I will have to get the images with the following function. Additionally, I have incorporated a greyscaler into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def load_databatch(data_folder, idx, img_size=32):\n",
    "    data_file = os.path.join(data_folder, 'train_data_batch_' + str(idx))\n",
    "    d = unpickle(data_file)\n",
    "    x = d['data']\n",
    "    y = d['labels']\n",
    "    mean_image = d['mean']\n",
    "\n",
    "    x = x / np.float32(255)\n",
    "    mean_image = mean_image / np.float32(255)\n",
    "\n",
    "    # Labels are indexed from 1, shift it so that indexes start at 0\n",
    "    y = [i - 1 for i in y]\n",
    "    data_size = x.shape[0]\n",
    "\n",
    "    x -= mean_image\n",
    "\n",
    "    img_size2 = img_size * img_size\n",
    "\n",
    "    x = np.dstack((x[:, :img_size2], x[:, img_size2:2 * img_size2], x[:, 2 * img_size2:]))\n",
    "    x = x.reshape((x.shape[0], img_size, img_size, 3)).transpose(0, 3, 1, 2)\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:data_size, :, :, :]\n",
    "    Y_train = y[0:data_size]\n",
    "    X_train_flip = X_train[:, :, :, ::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train, X_train_flip), axis=0)\n",
    "    Y_train = np.concatenate((Y_train, Y_train_flip), axis=0)\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train.astype(np.float32),\n",
    "        Y_train=np.array(Y_train, dtype=np.int32),\n",
    "        mean=mean_image.astype(np.float32)\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's superimpose them with each other!\n",
    "\n",
    "Firstly here is the way I loaded the mnist pictures as it was done before [here](./vae.ipynb). The data is loaded and resized to 32x32 to fit the Image-Net Data. \n",
    "I have once again split the data into a training and validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32, 32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=True, download=True, transform=transform\n",
    ")\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I am unpickleing one batch and doing the calculations that were previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = load_databatch(\"./input/Imagenet32_train\", 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets split the channels into matrices to then greyscale them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape is now:  (256232, 3, 32, 32)\n",
      "The shape is now:  (256232, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "image_data = batch_1[\"X_train\"]\n",
    "\n",
    "# Get the number of images\n",
    "num_images = len(image_data)\n",
    "\n",
    "# Initialize an empty array to store the reshaped grayscale images\n",
    "grayscale_images = np.empty((num_images, 32, 32), dtype=np.float32)\n",
    "\n",
    "print(\"The shape is now: \",image_data.shape)\n",
    "\n",
    "# Reshape the images\n",
    "for i, image in enumerate(image_data):\n",
    "    # Reshape the image to match the color channel dimensions (3x32x32)\n",
    "    image = np.reshape(image, (3, 32, 32))\n",
    "\n",
    "    # Split the image into separate color channels\n",
    "    red_channel = image[0]\n",
    "    green_channel = image[1]\n",
    "    blue_channel = image[2]\n",
    "\n",
    "    # Combine the color channels weighted by their respective coefficients to form the grayscale image - these values are taken from recommedations from ChatGPT\n",
    "    grayscale_image = 0.2989 * red_channel + 0.5870 * green_channel + 0.1140 * blue_channel\n",
    "\n",
    "    # Normalize the grayscale image to the range [0, 1]\n",
    "    grayscale_image /= 255.0\n",
    "\n",
    "    # Store the grayscale image in the array\n",
    "    grayscale_images[i] = grayscale_image\n",
    "\n",
    "# Store the reshaped grayscale images in a new entry in the dictionary\n",
    "batch_1[\"X_train_gray\"] = grayscale_images\n",
    "print(\"The shape is now: \",grayscale_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seemed to have worked. Now, lets move on to overlaying them. The following still needs proper testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlayed Images: 60000\n",
      "Not Overlayed Images: 196232\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "mnist_data = datasets.MNIST(root='./input', train=True, download=True, transform=transform)\n",
    "\n",
    "# Access the background images from the dictionary\n",
    "background_images = batch_1[\"X_train_gray\"]\n",
    "\n",
    "# Create a list to store the overlaid images\n",
    "target_images = []\n",
    "overlayed_count = 0\n",
    "\n",
    "# Create a list to store the background images (and load them)\n",
    "bg_images = []\n",
    "not_overlayed_count = 0\n",
    "\n",
    "# Shuffle the indices of the MNIST dataset\n",
    "mnist_indices = list(range(len(mnist_data)))\n",
    "random.shuffle(mnist_indices)\n",
    "\n",
    "# Assuming background_images and mnist_data are lists\n",
    "for i in range(len(background_images)):\n",
    "    background_image = background_images[i]\n",
    "    random_overlay = random.random() < 0.5  # Randomly choose whether to overlay MNIST image or not\n",
    "\n",
    "    if random_overlay and overlayed_count < len(mnist_data):  # Overlay MNIST image if there are MNIST images left\n",
    "        random_index = mnist_indices[overlayed_count]  # Use the next randomly shuffled index\n",
    "        mnist_image, _ = mnist_data[random_index]\n",
    "\n",
    "        # Invert the MNIST image\n",
    "        mnist_image = 1.0 - mnist_image\n",
    "\n",
    "        # Convert background image to tensor and move it to the CUDA device\n",
    "        background_tensor = torch.tensor(background_image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Resize the background image to match the MNIST image dimensions\n",
    "        resized_background_tensor = transforms.Resize((32, 32))(background_tensor)\n",
    "\n",
    "        # Resize MNIST image to match the background image dimensions\n",
    "        mnist_resized = transforms.Resize((32, 32))(mnist_image)\n",
    "\n",
    "        overlaid_tensor = resized_background_tensor + mnist_resized.to(device)\n",
    "\n",
    "        target_images.append(overlaid_tensor)  # Store information that the image was overlaid\n",
    "        overlayed_count += 1\n",
    "\n",
    "    else:\n",
    "        background_tensor = torch.tensor(background_image).unsqueeze(0).to(device)\n",
    "        resized_background_tensor = transforms.Resize((32, 32))(background_tensor)\n",
    "\n",
    "        bg_images.append(resized_background_tensor)  # Store information that the image was not overlaid\n",
    "        not_overlayed_count += 1\n",
    "\n",
    "# Print the counts of overlaid and non-overlaid images\n",
    "print(f\"Overlayed Images: {overlayed_count}\")\n",
    "print(f\"Not Overlayed Images: {not_overlayed_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I was a not so smart person before and started concatenating the images instead of using element wise addition, here is a constant reminder that I have now fixed this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of background images: 196232\n",
      "Number of overlaid images: 60000\n",
      "Background image dimensions: torch.Size([1, 32, 32])\n",
      "Overlaid image dimensions: torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "def check_image_dimensions(bg_images, target_images):\n",
    "    background_count = len(bg_images)\n",
    "    overlaid_count = len(target_images)\n",
    "\n",
    "    if background_count > 0:\n",
    "        background_dim = bg_images[0].shape\n",
    "    else:\n",
    "        background_dim = None\n",
    "\n",
    "    if overlaid_count > 0:\n",
    "        overlaid_dim = target_images[0].shape\n",
    "    else:\n",
    "        overlaid_dim = None\n",
    "\n",
    "    return background_count, overlaid_count, background_dim, overlaid_dim\n",
    "\n",
    "# Usage example:\n",
    "background_count, overlaid_count, background_dim, overlaid_dim = check_image_dimensions(bg_images, target_images)\n",
    "\n",
    "print(f\"Number of background images: {background_count}\")\n",
    "print(f\"Number of overlaid images: {overlaid_count}\")\n",
    "print(f\"Background image dimensions: {background_dim}\")\n",
    "print(f\"Overlaid image dimensions: {overlaid_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I check if anything is not a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_images:\n",
    "    if not torch.is_tensor(target) or target.shape != (1, 32, 32):\n",
    "        print(\"Non-tensor overlaid image found!\")\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for background in bg_images:\n",
    "    if not torch.is_tensor(background):\n",
    "        print(\"Non-tensor background found!\")\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the data-loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class OverlaidDataset(Dataset):\n",
    "    def __init__(self, target_images, bg_images):\n",
    "        self.target_images = target_images\n",
    "        self.bg_images = bg_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bg_images)  # Use the length of the background images list as the dataset length. I got more background images. \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_image = self.target_images[index % len(self.target_images)]  # Cycle through target images\n",
    "        bg_image = self.bg_images[index]\n",
    "\n",
    "        return target_image, bg_image\n",
    "\n",
    "# Create the overlaid dataset\n",
    "overlaid_dataset = OverlaidDataset(target_images, bg_images)\n",
    "\n",
    "# Create the dataloader\n",
    "batch_size = 64\n",
    "shuffle = True  # Shuffle the dataset to mix target and background images\n",
    "overlaid_dataloader = DataLoader(overlaid_dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking again that I still have the right data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_image is not a tensor of size 32x32. Instead it is:  torch.Size([15, 1, 32, 32])\n",
      "bg_image is not a tensor of size 32x32. Instead it is:  torch.Size([15, 1, 32, 32])\n",
      "There are  10417 batches with the target and  10417 without.\n"
     ]
    }
   ],
   "source": [
    "count_bg = 0\n",
    "count_target = 0\n",
    "\n",
    "for target_image, bg_image in overlaid_dataloader:\n",
    "    # Check if target_image is a tensor of size 32x32\n",
    "    if isinstance(target_image, torch.Tensor) and target_image.shape == (64, 1, 32, 32):\n",
    "        count_target += 1\n",
    "    else:\n",
    "        print(\"target_image is not a tensor of size 32x32. Instead it is: \", target_image.shape)\n",
    "\n",
    "    # Check if bg_image is a tensor of size 32x32\n",
    "    if isinstance(bg_image, torch.Tensor) and bg_image.shape == (64, 1, 32, 32):\n",
    "        count_bg += 1\n",
    "    else:\n",
    "        print(\"bg_image is not a tensor of size 32x32. Instead it is: \", bg_image.shape)\n",
    "\n",
    "print(\"There are \", count_target, \"batches with the target and \", count_bg, \"without.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am creating a training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:11, 277.30it/s]                          \n",
      "3067it [00:02, 1443.81it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 184.0886, Val Loss: 155.3552\n",
      "Epoch 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:09, 324.47it/s]                          \n",
      "3067it [00:02, 1447.34it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 152.1240, Val Loss: 150.4917\n",
      "Epoch 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:09, 317.70it/s]                          \n",
      "3067it [00:02, 1421.93it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 148.9702, Val Loss: 148.5573\n",
      "Epoch 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:09, 322.63it/s]                          \n",
      "3067it [00:02, 1447.99it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 147.3292, Val Loss: 147.3800\n",
      "Epoch 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:09, 322.51it/s]                          \n",
      "3067it [00:02, 1440.53it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 146.1827, Val Loss: 146.7699\n",
      "Epoch 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:09, 323.79it/s]                          \n",
      "3067it [00:02, 1441.24it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 145.2640, Val Loss: 144.7778\n",
      "Epoch 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:09, 324.33it/s]                          \n",
      "3067it [00:02, 1448.31it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 144.4817, Val Loss: 144.0898\n",
      "Epoch 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:09, 312.55it/s]                          \n",
      "3067it [00:02, 1412.84it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 143.8773, Val Loss: 143.7687\n",
      "Epoch 9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:09, 317.45it/s]                          \n",
      "3067it [00:02, 1485.95it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 143.1943, Val Loss: 142.3597\n",
      "Epoch 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [00:09, 318.83it/s]                          \n",
      "3067it [00:02, 1453.19it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 142.7570, Val Loss: 141.9952\n",
      "TRAINING COMPLETE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = cVAE(latent_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "train_loss_list = []  # List to store train losses\n",
    "val_loss_list = []  # List to store validation losses\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1} of {num_epochs}\")\n",
    "    # Train the model\n",
    "    train_loss_target, train_loss_bg = train(model, overlaid_dataloader, overlaid_dataset, device, optimizer, criterion)\n",
    "    \n",
    "    # Validate the model\n",
    "    val_loss, recon_images = validate(model, overlaid_dataloader, overlaid_dataset, device, criterion)\n",
    "    \n",
    "    # Appending the loss values to a list to allow for visualizations:\n",
    "\n",
    "    train_loss_list.append(train_loss_target)\n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "\n",
    "    # Print the losses\n",
    "    print(f\"Train Loss: {train_loss_target:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print('TRAINING COMPLETE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/code/cvae_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But did this work?\n",
    "\n",
    "\n",
    "Let's start by visually inspecting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_outputs(model, dataloader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tg, bg in dataloader:  # Iterate over batches, we don't need target_image in this version\n",
    "            # Forward pass to get all the outputs\n",
    "            z_mean, z_log_var, s_mean, s_log_var, z_mean_bg, z_log_var_bg, reconstructed_data_target, reconstructed_data_bg = model(tg, bg)\n",
    "\n",
    "            # Convert tensors to numpy arrays for visualization\n",
    "            target_images = tg.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "            recon_images_target = reconstructed_data_target.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "            recon_images_bg = reconstructed_data_bg.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "            # Select one image from the batch (index 0)\n",
    "            target_image = target_images[0]\n",
    "            recon_image_target = recon_images_target[0]\n",
    "            recon_image_bg = recon_images_bg[0]\n",
    "            original_bg = bg[0].cpu().numpy().transpose(1, 2, 0)  # Assuming the bg tensor has shape (batch_size, C, H, W)\n",
    "\n",
    "            # Plot the original and reconstructed images for target and background\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n",
    "            axes[0].imshow(original_bg)\n",
    "            axes[0].set_title(\"Original Background\")\n",
    "            axes[0].axis(\"off\")\n",
    "\n",
    "            axes[1].imshow(target_image)\n",
    "            axes[1].set_title(\"Original Target\")\n",
    "            axes[1].axis(\"off\")\n",
    "\n",
    "            axes[2].imshow(recon_image_target)\n",
    "            axes[2].set_title(\"Reconstructed (Target)\")\n",
    "            axes[2].axis(\"off\")\n",
    "\n",
    "            axes[3].imshow(recon_image_bg)\n",
    "            axes[3].set_title(\"Reconstructed (Background)\")\n",
    "            axes[3].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # We have visualized one image, so break the loop\n",
    "            break\n",
    "\n",
    "# Assuming you already have a validation dataloader named `val_dataloader`\n",
    "visualize_outputs(model, overlaid_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have figured out why the visuals were off - the background on the mnist images was already at max, therefore the background didnt show. This should fix it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
