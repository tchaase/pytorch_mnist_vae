{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Variational Autoencoder for the MNIST-Data-Set\n",
    "\n",
    "Tobias Haase\n",
    "\n",
    "I am slightly orienting myself on a paper from [Abid & Zou (2019)](https://arxiv.org/abs/1902.04601)\n",
    "## Set Up\n",
    "Firstly I am loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import imageio \n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/tchaase/Documents/Universitaet/Master-Arbeit/pytorch_mnist_VAE/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the [vae](vae.ipynb) I will firstly also define the utility functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "        counter += 1  #There is a counter that is initialized at 0, which goes up for every batch?\n",
    "        data = data[0]\n",
    "        data = data.to(device) # All computations are supposed to be computed on my GPU. \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        loss.backward()  # Using the loss, backpropagation occurs. Thus, all the tensors that will be connected to this, will be involved in this computation. \n",
    "        running_loss += loss.item() #This here defined for every step along the way, how high is the loss. \n",
    "        optimizer.step()  #That with a gradient will be updated in one step according to the documentation. \n",
    "    train_loss = running_loss / counter \n",
    "    return train_loss #The function returns only the training loss. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the validation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "            counter += 1\n",
    "            data= data[0]\n",
    "            data = data.to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
    "                recon_images = reconstruction\n",
    "    val_loss = running_loss / counter\n",
    "    return val_loss, recon_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again I also want to save images. The goal would be to have disentangled pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image = transforms.ToPILImage() #conversion of a tensor to an image, this will later be used to generate .gif images!\n",
    "\n",
    "#The following function converts the PILutilityges to .gif files. The accepted data are numpy arrays!\n",
    "def image_to_vid(images):     \n",
    "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
    "    imageio.mimsave('../outputs/generated_images.gif', imgs)\n",
    "\n",
    "#This function is equal to the function above, but for outputs of the VAE\n",
    "def save_reconstructed_images(recon_images, epoch):\n",
    "    save_image(recon_images.cpu(), f\"../outputs/output{epoch}.jpg\")  #The save image function comes from torchvision.utilis!\n",
    "\n",
    "#Finally, here the training and validation losses are saved into a plot! This is done via matplotlib. \n",
    "def save_loss_plot(train_loss, valid_loss):\n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color='orange', label='train loss')\n",
    "    plt.plot(valid_loss, color='red', label='validataion loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('../outputs/loss.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lastly I will define some parameters that I will use before I define the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3 # (4, 4) kernel\n",
    "init_channels = 8 # initial number of filters, first layers output. \n",
    "image_channels = 1 # MNIST images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling\n",
    "stride = 2\n",
    "channels = 1  #working with grayscale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EncoderZ(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, kernel_size= kernel_size, stride= stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size= kernel_size, stride= stride, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
    "        self.fc_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        z_mean = self.fc_mean(x)\n",
    "        z_log_var = self.fc_log_var(x)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class EncoderS(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(EncoderS, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, kernel_size= kernel_size, stride= stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size= kernel_size , stride= stride, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
    "        self.fc_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        s_mean = self.fc_mean(x)\n",
    "        s_log_var = self.fc_log_var(x)\n",
    "        return s_mean, s_log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 7 * 7 * 64)\n",
    "        self.conv1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(32, channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc1(z))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class cVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(cVAE, self).__init__()\n",
    "        self.encoder_z = EncoderZ(latent_dim, channels)\n",
    "        self.encoder_s = EncoderS(latent_dim, channels)\n",
    "        self.decoder = Decoder(latent_dim, channels)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + epsilon * std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, I can't directly access the images to superimpose them. Therefore, I will have to get the images with the following function. Additionally, I have incorporated a greyscaler into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def load_databatch(data_folder, idx, img_size=32):\n",
    "    data_file = os.path.join(data_folder, 'train_data_batch_' + str(idx))\n",
    "    d = unpickle(data_file)\n",
    "    x = d['data']\n",
    "    y = d['labels']\n",
    "    mean_image = d['mean']\n",
    "\n",
    "    x = x / np.float32(255)\n",
    "    mean_image = mean_image / np.float32(255)\n",
    "\n",
    "    # Labels are indexed from 1, shift it so that indexes start at 0\n",
    "    y = [i - 1 for i in y]\n",
    "    data_size = x.shape[0]\n",
    "\n",
    "    x -= mean_image\n",
    "\n",
    "    img_size2 = img_size * img_size\n",
    "\n",
    "    x = np.dstack((x[:, :img_size2], x[:, img_size2:2 * img_size2], x[:, 2 * img_size2:]))\n",
    "    x = x.reshape((x.shape[0], img_size, img_size, 3)).transpose(0, 3, 1, 2)\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:data_size, :, :, :]\n",
    "    Y_train = y[0:data_size]\n",
    "    X_train_flip = X_train[:, :, :, ::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train, X_train_flip), axis=0)\n",
    "    Y_train = np.concatenate((Y_train, Y_train_flip), axis=0)\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train.astype(np.float32),\n",
    "        Y_train=np.array(Y_train, dtype=np.int32),\n",
    "        mean=mean_image.astype(np.float32)\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's superimpose them with each other!\n",
    "\n",
    "Firstly here is the way I loaded the mnist pictures as it was done before [here](./vae.ipynb). The data is loaded and resized to 32x32 to fit the Image-Net Data. \n",
    "I have once again split the data into a training and validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32, 32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=True, download=True, transform=transform\n",
    ")\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I am unpickleing one batch and doing the calculations that were previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = load_databatch(\"./input/Imagenet32_train\", 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets split the channels into matrices to then greyscale them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = batch_1[\"X_train\"]\n",
    "\n",
    "# Get the number of images\n",
    "num_images = len(image_data)\n",
    "\n",
    "# Initialize an empty array to store the reshaped grayscale images\n",
    "grayscale_images = np.empty((num_images, 32, 32), dtype=np.float32)\n",
    "\n",
    "print(\"The shape is now: \",image_data.shape)\n",
    "\n",
    "# Reshape the images\n",
    "for i, image in enumerate(image_data):\n",
    "    # Reshape the image to match the color channel dimensions (3x32x32)\n",
    "    image = np.reshape(image, (3, 32, 32))\n",
    "\n",
    "    # Split the image into separate color channels\n",
    "    red_channel = image[0]\n",
    "    green_channel = image[1]\n",
    "    blue_channel = image[2]\n",
    "\n",
    "    # Combine the color channels weighted by their respective coefficients to form the grayscale image - these values are taken from recommedations from ChatGPT\n",
    "    grayscale_image = 0.2989 * red_channel + 0.5870 * green_channel + 0.1140 * blue_channel\n",
    "\n",
    "    # Normalize the grayscale image to the range [0, 1]\n",
    "    grayscale_image /= 255.0\n",
    "\n",
    "    # Store the grayscale image in the array\n",
    "    grayscale_images[i] = grayscale_image\n",
    "\n",
    "# Store the reshaped grayscale images in a new entry in the dictionary\n",
    "batch_1[\"X_train_gray\"] = grayscale_images\n",
    "print(\"The shape is now: \",greenscale_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seemed to have worked. Now, lets move on to overlaying them. The following still needs proper testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MNIST images to numpy arrays\n",
    "x_train_mnist = np.array(mnist_dataset.data)\n",
    "y_train_mnist = np.array(mnist_dataset.targets)\n",
    "\n",
    "# Resize MNIST images to match the size of the pictures in the dictionary\n",
    "num_images = len(batch_1[\"X_train\"])\n",
    "x_train_resized = np.array([np.resize(img, (32, 32)) for img in x_train_mnist[:num_images]])\n",
    "\n",
    "# Overlay MNIST images on the pictures in the dictionary\n",
    "for i in range(num_images):\n",
    "    x, y = 10, 10  # Location to overlay the MNIST image (adjust as needed)\n",
    "    alpha = 0.5  # Opacity of the overlay\n",
    "\n",
    "    # Overlay the MNIST image onto the picture\n",
    "    batch_1[\"X_train\"][i][x:x+28, y:y+28] = alpha * x_train_resized[i] + (1 - alpha) * batch_1[\"X_train\"][i][x:x+28, y:y+28]\n",
    "\n",
    "    # Update the corresponding indices for the overlayed images\n",
    "    batch_1[\"y_train\"][i] = y_train_mnist[i]\n",
    "\n",
    "# Print the dimensions of one picture in the dictionary\n",
    "print(\"Dimensions of a picture after overlaying MNIST dataset:\", batch_1[\"X_train\"][0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
