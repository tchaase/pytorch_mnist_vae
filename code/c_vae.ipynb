{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Variational Autoencoder for the MNIST-Data-Set\n",
    "\n",
    "Tobias Haase\n",
    "\n",
    "I am slightly orienting myself on a paper from [Abid & Zou (2019)](https://arxiv.org/abs/1902.04601)\n",
    "## Set Up\n",
    "Firstly I am loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import imageio \n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/tchaase/Documents/Universitaet/Master-Arbeit/pytorch_mnist_VAE/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the [vae](vae.ipynb) I will firstly also define the utility functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, z_mu, z_logvar, s_mu=None, s_logvar=None):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: reconstruction loss\n",
    "    :param z_mu: mean from the latent vector of encoder_z\n",
    "    :param z_logvar: log variance from the latent vector of encoder_z\n",
    "    :param s_mu: mean from the latent vector of encoder_s (optional)\n",
    "    :param s_logvar: log variance from the latent vector of encoder_s (optional)\n",
    "    \"\"\"\n",
    "    BCE = bce_loss\n",
    "    KLD_z = -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp())\n",
    "    if s_mu is not None and s_logvar is not None:\n",
    "        KLD_s = -0.5 * torch.sum(1 + s_logvar - s_mu.pow(2) - s_logvar.exp())\n",
    "        return BCE + KLD_z + KLD_s\n",
    "    else:\n",
    "        return BCE + KLD_z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i, (data, is_overlayed) in tqdm(enumerate(dataloader), total=len(dataset)):\n",
    "        data = data.to(device)\n",
    "        overlay_status = is_overlayed.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for j in range(data.size(0)):   #This seems weird, but I got errors related to the bce loss parameters being wrong, i.e. its both [1,32,32], but the input is [64,1,32,32, therefore I choose to iterate through participant and accumulate the gradient this way]\n",
    "            single_data = data[j]\n",
    "            single_overlay_status = overlay_status[j]\n",
    "\n",
    "            if single_overlay_status:\n",
    "                z_mean, z_log_var, s_mean, s_log_var, reconstructed_data = model(single_data)\n",
    "                bce_loss = criterion(reconstructed_data, single_data)\n",
    "                loss = final_loss(bce_loss, z_mean, z_log_var, s_mean, s_log_var)\n",
    "            else:\n",
    "                z_mean, z_log_var,reconstructed_data = model(single_data) #unsqueeze added to make the dimensions for the output comparable to the input)\n",
    "                bce_loss = criterion(reconstructed_data, single_data)\n",
    "                loss = final_loss(bce_loss, z_mean, z_log_var, None, None)  # Pass None for s_mean and s_log_var\n",
    "\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        counter += data.size(0)\n",
    "\n",
    "    train_loss = running_loss / counter\n",
    "    return train_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the validation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "            counter += 1\n",
    "            data = data[0]\n",
    "            data = data.to(device)\n",
    "            reconstruction, z_mu, z_logvar, s_mu, s_logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, z_mu, z_logvar, s_mu, s_logvar)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
    "                recon_images = reconstruction\n",
    "    val_loss = running_loss / counter\n",
    "    return val_loss, recon_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m exp \u001b[39m=\u001b[39m (exp\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Print the flattened tensor shape\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mprint\u001b[39m(exp\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lastly I will define some parameters that I will use before I define the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4 \n",
    "init_channels = 8 # initial number of filters, first layers output. \n",
    "image_channels = 1 # MNIST images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling\n",
    "stride = 2\n",
    "channels = 1  #working with grayscale. \n",
    "lr = 0.001\n",
    "same = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "    \n",
    "class EncoderZ(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.fc1 = nn.Linear(init_channels*2* 8 * 8, 256) # t applies a linear transformation to the input data, which means it computes the dot product of the input and weight tensors, adds the bias term, and produces the output.\n",
    "        self.fc_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h_flat = h.view(h.size(0), -1)  # Flatten the tensor\n",
    "        h = F.relu(self.fc1(h_flat))\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_log_var = self.fc_log_var(h)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class EncoderS(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(EncoderS, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.fc1 = nn.Linear(init_channels * 2*8 * 8, 256)\n",
    "        self.fc_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h_flat = h.view(h.size(0), -1)  # Flatten the tensor\n",
    "        h = F.relu(self.fc1(h_flat))\n",
    "        s_mean = self.fc_mean(h)\n",
    "        s_log_var = self.fc_log_var(h)\n",
    "        return s_mean, s_log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, 1024)  # Adjust the output size of fc2\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=init_channels, out_channels=init_channels*2, kernel_size=4, stride=2, padding=1)  # Set input channels to 1\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=init_channels*2, out_channels = image_channels, kernel_size=4, stride=2, padding= 1 )\n",
    "\n",
    "    def forward(self, zs):\n",
    "        x = F.relu(self.fc1(zs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(-1, 1, 16, 16)  # Reshape to a 4-dimensional tensor. I changed the first value to 1 recently, to have the input and output have equal dimensions.\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class cVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(cVAE, self).__init__()\n",
    "        self.encoder_z = EncoderZ(latent_dim)\n",
    "        self.encoder_s = EncoderS(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.overlay_status = None \n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + epsilon * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var = self.encoder_z(x)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        \n",
    "        if self.overlay_status: \n",
    "            mean, s_log_var = self.encoder_s(x)\n",
    "            s = self.reparameterize(s_mean, s_log_var)\n",
    "            zs = torch.cat([z, s], dim=1)\n",
    "        else:\n",
    "            empty_vector = torch.empty_like(z).detach()\n",
    "            zs = torch.cat([z,empty_vector], dim = 1)\n",
    "            \n",
    "        reconstructed_data = self.decoder(zs)\n",
    "        \n",
    "        if self.overlay_status:\n",
    "            return z_mean, z_log_var, s_mean, s_log_var, reconstructed_data\n",
    "        else:\n",
    "            return z_mean, z_log_var, reconstructed_data\n",
    "        \n",
    "    def get_overlay_status(self):\n",
    "        return self.overlay_status"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, I can't directly access the images to superimpose them. Therefore, I will have to get the images with the following function. Additionally, I have incorporated a greyscaler into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def load_databatch(data_folder, idx, img_size=32):\n",
    "    data_file = os.path.join(data_folder, 'train_data_batch_' + str(idx))\n",
    "    d = unpickle(data_file)\n",
    "    x = d['data']\n",
    "    y = d['labels']\n",
    "    mean_image = d['mean']\n",
    "\n",
    "    x = x / np.float32(255)\n",
    "    mean_image = mean_image / np.float32(255)\n",
    "\n",
    "    # Labels are indexed from 1, shift it so that indexes start at 0\n",
    "    y = [i - 1 for i in y]\n",
    "    data_size = x.shape[0]\n",
    "\n",
    "    x -= mean_image\n",
    "\n",
    "    img_size2 = img_size * img_size\n",
    "\n",
    "    x = np.dstack((x[:, :img_size2], x[:, img_size2:2 * img_size2], x[:, 2 * img_size2:]))\n",
    "    x = x.reshape((x.shape[0], img_size, img_size, 3)).transpose(0, 3, 1, 2)\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:data_size, :, :, :]\n",
    "    Y_train = y[0:data_size]\n",
    "    X_train_flip = X_train[:, :, :, ::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train, X_train_flip), axis=0)\n",
    "    Y_train = np.concatenate((Y_train, Y_train_flip), axis=0)\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train.astype(np.float32),\n",
    "        Y_train=np.array(Y_train, dtype=np.int32),\n",
    "        mean=mean_image.astype(np.float32)\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's superimpose them with each other!\n",
    "\n",
    "Firstly here is the way I loaded the mnist pictures as it was done before [here](./vae.ipynb). The data is loaded and resized to 32x32 to fit the Image-Net Data. \n",
    "I have once again split the data into a training and validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32, 32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=True, download=True, transform=transform\n",
    ")\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I am unpickleing one batch and doing the calculations that were previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = load_databatch(\"./input/Imagenet32_train\", 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets split the channels into matrices to then greyscale them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape is now:  (256232, 3, 32, 32)\n",
      "The shape is now:  (256232, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "image_data = batch_1[\"X_train\"]\n",
    "\n",
    "# Get the number of images\n",
    "num_images = len(image_data)\n",
    "\n",
    "# Initialize an empty array to store the reshaped grayscale images\n",
    "grayscale_images = np.empty((num_images, 32, 32), dtype=np.float32)\n",
    "\n",
    "print(\"The shape is now: \",image_data.shape)\n",
    "\n",
    "# Reshape the images\n",
    "for i, image in enumerate(image_data):\n",
    "    # Reshape the image to match the color channel dimensions (3x32x32)\n",
    "    image = np.reshape(image, (3, 32, 32))\n",
    "\n",
    "    # Split the image into separate color channels\n",
    "    red_channel = image[0]\n",
    "    green_channel = image[1]\n",
    "    blue_channel = image[2]\n",
    "\n",
    "    # Combine the color channels weighted by their respective coefficients to form the grayscale image - these values are taken from recommedations from ChatGPT\n",
    "    grayscale_image = 0.2989 * red_channel + 0.5870 * green_channel + 0.1140 * blue_channel\n",
    "\n",
    "    # Normalize the grayscale image to the range [0, 1]\n",
    "    grayscale_image /= 255.0\n",
    "\n",
    "    # Store the grayscale image in the array\n",
    "    grayscale_images[i] = grayscale_image\n",
    "\n",
    "# Store the reshaped grayscale images in a new entry in the dictionary\n",
    "batch_1[\"X_train_gray\"] = grayscale_images\n",
    "print(\"The shape is now: \",grayscale_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seemed to have worked. Now, lets move on to overlaying them. The following still needs proper testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tchaase/miniconda3/envs/pytorch11.7/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlayed Images: 30000\n",
      "Not Overlayed Images: 226232\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "mnist_data = datasets.MNIST(root='./input', train=True, download=True, transform=transform)\n",
    "\n",
    "# Access the background images from the dictionary\n",
    "background_images = batch_1[\"X_train_gray\"]\n",
    "\n",
    "# Create a list to store the overlaid images\n",
    "overlaid_images = []\n",
    "overlayed_count = 0\n",
    "not_overlayed_count = 0\n",
    "\n",
    "\n",
    "# Work in progress, made two channels instead to actually get it to run but should remove channels. Sometimes in code two channels show up I dont understand why. \n",
    "\n",
    "for i in range(len(background_images)):\n",
    "    background_image = background_images[i]\n",
    "    if i < len(mnist_data) // 2:  # Overlay MNIST image for half of the background images\n",
    "        random_index = random.choice([0, 1, 2])  # Choose random digit 0, 1, or 2\n",
    "        mnist_image, _ = mnist_data[random_index]\n",
    "\n",
    "        # Convert background image to tensor and move it to the CUDA device\n",
    "        background_tensor = torch.tensor(background_image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Resize the background image to match the MNIST image dimensions\n",
    "        resized_background_tensor = transforms.Resize((32, 32))(background_tensor)\n",
    "\n",
    "        # Remove extra channel if present\n",
    "        if resized_background_tensor.shape[0] == 2:\n",
    "            resized_background_tensor = resized_background_tensor[0]\n",
    "\n",
    "        # Resize MNIST image to match the background image dimensions\n",
    "        mnist_resized = transforms.Resize((32, 32))(mnist_image)\n",
    "\n",
    "        overlaid_tensor = torch.cat([resized_background_tensor, mnist_resized.to(device)], dim=0)\n",
    "        overlaid_images.append((overlaid_tensor, True))  # Store information that the image was overlaid\n",
    "        overlayed_count += 1\n",
    "    else:\n",
    "        background_tensor = torch.tensor(background_image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Remove extra channel if present\n",
    "        if background_tensor.shape[0] == 2:\n",
    "            background_tensor = background_tensor[0]\n",
    "\n",
    "        overlaid_images.append((background_tensor, False))  # Store information that the image was not overlaid\n",
    "        not_overlayed_count += 1\n",
    "        \n",
    "\n",
    "\n",
    "# Print the counts of overlaid and non-overlaid images\n",
    "print(f\"Overlayed Images: {overlayed_count}\")\n",
    "print(f\"Not Overlayed Images: {not_overlayed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for background, is_overlayed in overlaid_images:\n",
    "    if not torch.is_tensor(background):\n",
    "        print(\"Non-tensor background found!\")\n",
    "        # Handle the case when a non-tensor background is encountered\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the data-loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlaidDataset(Dataset):\n",
    "    def __init__(self, overlaid_images, channel_index=0):\n",
    "        self.overlaid_images = overlaid_images\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.channel_index = channel_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.overlaid_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, is_overlayed = self.overlaid_images[index]\n",
    "\n",
    "        # Select the specified channel\n",
    "        image = image.narrow(0, self.channel_index, 1)\n",
    "\n",
    "        return image, is_overlayed\n",
    "\n",
    "\n",
    "# Create the overlaid dataset\n",
    "overlaid_dataset = OverlaidDataset(overlaid_images)\n",
    "\n",
    "# Create the dataloader\n",
    "batch_size = 64\n",
    "overlaid_dataloader = DataLoader(overlaid_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking again that I still have the right data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_images, _ = next(iter(overlaid_dataloader))\n",
    "\n",
    "for idx, x in enumerate(dataloader_images):\n",
    "    if not torch.is_tensor(x):\n",
    "        print(\"Non-tensor image found! This is of the type:\", type(x))\n",
    "        # Handle the case when a non-tensor background is encountered\n",
    "    else:\n",
    "        # Process the background tensor\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am creating a training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256232 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256232 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x36 and 1024x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      7\u001b[0m     \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, overlaid_dataloader, overlaid_dataset, device, optimizer, criterion)\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Validate the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     val_loss, recon_images \u001b[39m=\u001b[39m validate(model, overlaid_dataloader, overlaid_dataset, device, criterion)\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, dataset, device, optimizer, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m     loss \u001b[39m=\u001b[39m final_loss(bce_loss, z_mean, z_log_var, s_mean, s_log_var)\n\u001b[1;32m     19\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     z_mean, z_log_var,reconstructed_data \u001b[39m=\u001b[39m model(single_data) \u001b[39m#unsqueeze added to make the dimensions for the output comparable to the input)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     bce_loss \u001b[39m=\u001b[39m criterion(reconstructed_data, single_data)\n\u001b[1;32m     22\u001b[0m     loss \u001b[39m=\u001b[39m final_loss(bce_loss, z_mean, z_log_var, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# Pass None for s_mean and s_log_var\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[39], line 75\u001b[0m, in \u001b[0;36mcVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 75\u001b[0m     z_mean, z_log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_z(x)\n\u001b[1;32m     76\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(z_mean, z_log_var)\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverlay_status: \n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[39], line 20\u001b[0m, in \u001b[0;36mEncoderZ.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(h))\n\u001b[1;32m     19\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(h)\n\u001b[0;32m---> 20\u001b[0m h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(h))\n\u001b[1;32m     21\u001b[0m z_mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_mean(h)\n\u001b[1;32m     22\u001b[0m z_log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_log_var(h)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch11.7/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x36 and 1024x256)"
     ]
    }
   ],
   "source": [
    "model = cVAE(latent_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    train_loss = train(model, overlaid_dataloader, overlaid_dataset, device, optimizer, criterion)\n",
    "    \n",
    "    # Validate the model\n",
    "    val_loss, recon_images = validate(model, overlaid_dataloader, overlaid_dataset, device, criterion)\n",
    "    \n",
    "    # Print the losses\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, is_overlayed in overlaid_dataset:\n",
    "    if not isinstance(is_overlayed, bool):\n",
    "        print(is_overlayed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
