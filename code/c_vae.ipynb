{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Variational Autoencoder for the MNIST-Data-Set\n",
    "\n",
    "Tobias Haase\n",
    "\n",
    "I am slightly orienting myself on a paper from [Abid & Zou (2019)](https://arxiv.org/abs/1902.04601)\n",
    "## Set Up\n",
    "Firstly I am loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import imageio \n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/tchaase/Documents/Universitaet/Master-Arbeit/pytorch_mnist_VAE/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the [vae](vae.ipynb) I will firstly also define the utility functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "        counter += 1  #There is a counter that is initialized at 0, which goes up for every batch?\n",
    "        data = data[0]\n",
    "        data = data.to(device) # All computations are supposed to be computed on my GPU. \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        loss.backward()  # Using the loss, backpropagation occurs. Thus, all the tensors that will be connected to this, will be involved in this computation. \n",
    "        running_loss += loss.item() #This here defined for every step along the way, how high is the loss. \n",
    "        optimizer.step()  #That with a gradient will be updated in one step according to the documentation. \n",
    "    train_loss = running_loss / counter \n",
    "    return train_loss #The function returns only the training loss. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the validation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "            counter += 1\n",
    "            data= data[0]\n",
    "            data = data.to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
    "                recon_images = reconstruction\n",
    "    val_loss = running_loss / counter\n",
    "    return val_loss, recon_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again I also want to save images. The goal would be to have disentangled pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image = transforms.ToPILImage() #conversion of a tensor to an image, this will later be used to generate .gif images!\n",
    "\n",
    "#The following function converts the PILutilityges to .gif files. The accepted data are numpy arrays!\n",
    "def image_to_vid(images):     \n",
    "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
    "    imageio.mimsave('../outputs/generated_images.gif', imgs)\n",
    "\n",
    "#This function is equal to the function above, but for outputs of the VAE\n",
    "def save_reconstructed_images(recon_images, epoch):\n",
    "    save_image(recon_images.cpu(), f\"../outputs/output{epoch}.jpg\")  #The save image function comes from torchvision.utilis!\n",
    "\n",
    "#Finally, here the training and validation losses are saved into a plot! This is done via matplotlib. \n",
    "def save_loss_plot(train_loss, valid_loss):\n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color='orange', label='train loss')\n",
    "    plt.plot(valid_loss, color='red', label='validataion loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('../outputs/loss.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lastly I will define some parameters that I will use before I define the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3 # (4, 4) kernel\n",
    "init_channels = 8 # initial number of filters, first layers output. \n",
    "image_channels = 1 # MNIST images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling\n",
    "stride = 2\n",
    "channels = 1  #working with grayscale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EncoderZ(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, kernel_size= kernel_size, stride= stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size= kernel_size, stride= stride, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
    "        self.fc_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        z_mean = self.fc_mean(x)\n",
    "        z_log_var = self.fc_log_var(x)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class EncoderS(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(EncoderS, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, kernel_size= kernel_size, stride= stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size= kernel_size , stride= stride, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
    "        self.fc_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        s_mean = self.fc_mean(x)\n",
    "        s_log_var = self.fc_log_var(x)\n",
    "        return s_mean, s_log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 7 * 7 * 64)\n",
    "        self.conv1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(32, channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc1(z))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class cVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(cVAE, self).__init__()\n",
    "        self.encoder_z = EncoderZ(latent_dim, channels)\n",
    "        self.encoder_s = EncoderS(latent_dim, channels)\n",
    "        self.decoder = Decoder(latent_dim, channels)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + epsilon * std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to get the corresponding data. The data for the MNIST data set is already loaded. I will do this in the next step. But firstly I have to convert the background iamges to gray scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "background_image_path = 'input/Imagenet32_train'\n",
    "\n",
    "# Iterate over the image files in the dataset directory\n",
    "for filename in os.listdir(background_image_path):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "        grayscale_image = image.convert('L')  # Convert to grayscale. L stands for a specific method, that extrats the luminance info. \n",
    "        grayscale_image.save(image_path)  # Overwrite the original image with the grayscale version\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, I can't directly access the images to superimpose them. Therefore, I will have to get the images with the following function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "\n",
    "def load_databatch(data_folder, idx, img_size=32):\n",
    "    data_file = os.path.join(data_folder, 'train_data_batch_')\n",
    "\n",
    "    d = unpickle(data_file + str(idx))\n",
    "    x = d['data']\n",
    "    y = d['labels']\n",
    "    mean_image = d['mean']\n",
    "\n",
    "    x = x/np.float32(255)\n",
    "    mean_image = mean_image/np.float32(255)\n",
    "\n",
    "    # Labels are indexed from 1, shift it so that indexes start at 0\n",
    "    y = [i-1 for i in y]\n",
    "    data_size = x.shape[0]\n",
    "\n",
    "    x -= mean_image\n",
    "\n",
    "    img_size2 = img_size * img_size\n",
    "\n",
    "    x = np.dstack((x[:, :img_size2], x[:, img_size2:2*img_size2], x[:, 2*img_size2:]))\n",
    "    x = x.reshape((x.shape[0], img_size, img_size, 3)).transpose(0, 3, 1, 2)\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:data_size, :, :, :]\n",
    "    Y_train = y[0:data_size]\n",
    "    X_train_flip = X_train[:, :, :, ::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train, X_train_flip), axis=0)\n",
    "    Y_train = np.concatenate((Y_train, Y_train_flip), axis=0)\n",
    "\n",
    "    return dict(\n",
    "        X_train=lasagne.utils.floatX(X_train),\n",
    "        Y_train=Y_train.astype('int32'),\n",
    "        mean=mean_image)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's superimpose them with each other!\n",
    "\n",
    "Firstly here is the way I loaded the mnist pictures as it was done before [here](./vae.ipynb). The data is loaded and resized to 32x32 to fit the Image-Net Data. \n",
    "I have once again split the data into a training and validation set. \n",
    "\n",
    "The imagenet data is loaded using a funcy approach with the ImageFolder class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Data transformations\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m transform \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m     torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mResize((\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m)),\n\u001b[1;32m      4\u001b[0m     torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      7\u001b[0m \u001b[39m# Load the MNIST dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m trainset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mMNIST(\n\u001b[1;32m      9\u001b[0m     root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./input\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "# Data transformations\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32, 32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=True, download=True, transform=transform\n",
    ")\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=False, download=True, transform=transform\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets obtain indexes so I can do the superimposing and remember which images are superimposed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m superimposed_images \u001b[39m=\u001b[39m []  \u001b[39m# List to store the superimposed images\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Load and superimpose ImageNet images onto the MNIST images\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m num_mnist_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(trainset)\n\u001b[1;32m      6\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m  \u001b[39m# Number of batches to load and superimpose\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_batches \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the ImageNet32 datasets\n",
    "# Obtaining the images:\n",
    "train_dir = \"./input/Imagenet32_train/\"\n",
    "val_dir = \"./input/Imagenet32_val\"\n",
    "\n",
    "# Superimpose the images\n",
    "superimposed_images = []  # List to store the superimposed images\n",
    "\n",
    "# Load and superimpose ImageNet images onto the MNIST images\n",
    "num_mnist_samples = len(trainset)\n",
    "num_batches = 5  # Number of batches to load and superimpose\n",
    "\n",
    "for batch_idx in range(1, num_batches + 1):\n",
    "    batch_data = load_databatch(data_folder='./input/Imagenet32_train', idx=batch_idx)\n",
    "    X_train = batch_data['X_train']\n",
    "\n",
    "    for i, idx in enumerate(superimpose_indices):\n",
    "        mnist_image = trainset.data[idx]  # Get the MNIST image\n",
    "        imagenet_image = X_train[i]  # Get the corresponding ImageNet image\n",
    "\n",
    "        # Perform the superimposition (example: averaging the pixel values)\n",
    "        superimposed_image = (mnist_image + imagenet_image) / 2.0\n",
    "\n",
    "        # Update the MNIST training set with the superimposed image\n",
    "        trainset.data[idx] = superimposed_image\n",
    "        superimposed_images.append(superimposed_image)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the images are superimposed, lets get the list of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists to tensors\n",
    "superimposed_images = torch.stack(superimposed_images)\n",
    "\n",
    "# Create labels for the superimposed images\n",
    "mnist_target_labels = torch.ones(len(superimposed_images), dtype=torch.long)\n",
    "\n",
    "# Create a dataset for the superimposed images\n",
    "mnist_superimposed_dataset = torch.utils.data.TensorDataset(superimposed_images, mnist_target_labels)\n",
    "\n",
    "# Create data loaders for the superimposed and non-superimposed images\n",
    "batch_size = 64\n",
    "superimposed_loader = DataLoader(mnist_superimposed_dataset, batch_size=batch_size, shuffle=True)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(\"Superimposed dataset size:\", len(mnist_superimposed_dataset))\n",
    "print(\"Training set size:\", len(trainset))\n",
    "print(\"Validation set size:\", len(testset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
