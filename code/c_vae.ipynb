{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Variational Autoencoder for the MNIST-Data-Set\n",
    "\n",
    "Tobias Haase\n",
    "\n",
    "I am slightly orienting myself on a paper from [Abid & Zou (2019)](https://arxiv.org/abs/1902.04601)\n",
    "## Set Up\n",
    "Firstly I am loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import imageio \n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/tchaase/Documents/Universitaet/Master-Arbeit/pytorch_mnist_VAE/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the [vae](vae.ipynb) I will firstly also define the utility functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, z_mu, z_logvar, s_mu, s_logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: reconstruction loss\n",
    "    :param z_mu: mean from the latent vector of encoder_z\n",
    "    :param z_logvar: log variance from the latent vector of encoder_z\n",
    "    :param s_mu: mean from the latent vector of encoder_s (optional)\n",
    "    :param s_logvar: log variance from the latent vector of encoder_s (optional)\n",
    "    \"\"\"\n",
    "    BCE = bce_loss\n",
    "    KLD_z = -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp())\n",
    "    if s_mu is not None and s_logvar is not None:\n",
    "        KLD_s = -0.5 * torch.sum(1 + s_logvar - s_mu.pow(2) - s_logvar.exp())\n",
    "        return BCE + KLD_z + KLD_s\n",
    "    else:\n",
    "        return BCE + KLD_z\n",
    "    \n",
    "    # Use cross entropy for mri images perhaps, more appropriate than bce. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss_bg = 0.0\n",
    "    running_loss_target = 0.0\n",
    "    counter = 0\n",
    "    for i, (target_data, background_data) in tqdm(enumerate(dataloader), total=len(dataset)):\n",
    "        counter += 1\n",
    "        target_data = target_data.to(device)\n",
    "        background_data = background_data.to(device)\n",
    "                       \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z_mean, z_log_var, s_mean, s_log_var, z_mean_bg, z_log_var_bg, reconstructed_data_target, reconstructed_data_bg = model(target_data, background_data)\n",
    "\n",
    "        # Section for the target images\n",
    "        bce_loss_target = criterion(reconstructed_data_target, target_data)\n",
    "        loss_target = final_loss(bce_loss_target, z_mean, z_log_var, s_mean, s_log_var)\n",
    "        running_loss_target += loss_target.item()\n",
    "        loss_target.backward \n",
    "\n",
    "        # Section for the background images\n",
    "        bce_loss_background = criterion(reconstructed_data_bg, background_data)\n",
    "        s_mean_bg, s_log_var_bg = None, None\n",
    "        loss_background = final_loss(bce_loss_background, z_mean_bg, z_log_var_bg, s_mean_bg, s_log_var_bg)\n",
    "        loss_background.backward()  # Using the loss, backpropagation occurs. Thus, all the tensors that will be connected to this, will be involved in this computation. \n",
    "        running_loss_bg += loss_background.item() #This here defined for every step along the way, how high is the loss. \n",
    "    \n",
    "        optimizer.step()\n",
    "        counter += target_data.size(0)\n",
    "    train_loss_target = running_loss_target / counter\n",
    "    train_loss_bg = running_loss_bg / counter\n",
    "    return train_loss_target, train_loss_bg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the validation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (target_data, background_data) in tqdm(enumerate(dataloader), total=len(dataset)):\n",
    "            counter += 1\n",
    "            target_data = target_data.to(device)\n",
    "            background_data = background_data.to(device)\n",
    "            counter += 1\n",
    "            \n",
    "            # Perform the forward pass\n",
    "            z_mean, z_log_var, s_mean, s_log_var, z_mean_bg, z_log_var_bg, reconstructed_data_target, reconstructed_data_bg = model(target_data, background_data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            bce_loss_target = criterion(reconstructed_data_target, target_data)\n",
    "            loss_target = final_loss(bce_loss_target, z_mean, z_log_var, s_mean, s_log_var)\n",
    "            running_loss += loss_target.item()\n",
    "            \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == len(dataset) // dataloader.batch_size - 1:\n",
    "                recon_images = reconstructed_data_target\n",
    "            \n",
    "    val_loss = running_loss / counter\n",
    "    return val_loss, recon_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lastly I will define some parameters that I will use before I define the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4 \n",
    "init_channels = 8 # initial number of filters, first layers output. \n",
    "image_channels = 1 # MNIST images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling\n",
    "stride = 2\n",
    "channels = 1  #working with grayscale. \n",
    "lr = 0.001\n",
    "same = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.randn(64, 16, 8, 8)\n",
    "h = tensor.view(64,-1)\n",
    "\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "latent_dim = 16\n",
    "\n",
    "\n",
    "class EncoderZ(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.fc1 = nn.Linear(init_channels*2* 8 * 8, 256) # t applies a linear transformation to the input data, which means it computes the dot product of the input and weight tensors, adds the bias term, and produces the output.\n",
    "        self.fc_mean = nn.Linear(256, (16))\n",
    "        self.fc_log_var = nn.Linear(256, (16))\n",
    "\n",
    "    def forward(self, x, batch_size):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(batch_size,-1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        z_mean = self.fc_mean(h)  \n",
    "        z_log_var = self.fc_log_var(h)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class EncoderS(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(EncoderS, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.fc1 = nn.Linear(init_channels * 2*8 * 8, 256)\n",
    "        self.fc_mean = nn.Linear(256, (16))\n",
    "        self.fc_log_var = nn.Linear(256, (16))\n",
    "\n",
    "    def forward(self, x, batch_size):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(batch_size,-1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        s_mean = self.fc_mean(h)\n",
    "        s_log_var = self.fc_log_var(h)\n",
    "        return s_mean, s_log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear((16* 2), 256)\n",
    "        self.fc2 = nn.Linear(256, 2048)  # Adjust the output size of fc2\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=init_channels*4, out_channels=init_channels*2, kernel_size=4, stride=2, padding=1)  # Set input channels to 1\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=init_channels*2, out_channels = image_channels, kernel_size=4, stride=2, padding= 1 )\n",
    "\n",
    "    def forward(self, zs, batch_size):\n",
    "        x = F.relu(self.fc1(zs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(batch_size, 32, 8, 8)  # Reshape to a 4-dimensional ten\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class cVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(cVAE, self).__init__()\n",
    "        self.encoder_z = EncoderZ(latent_dim)\n",
    "        self.encoder_s = EncoderS(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.overlay_status = None \n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + epsilon * std\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, target_image, bg_image):\n",
    "        batch_size = target_image.size(0) \n",
    "        z_mean, z_log_var = self.encoder_z(target_image, batch_size)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        s_mean, s_log_var = self.encoder_s(target_image, batch_size)\n",
    "        s = self.reparameterize(s_mean, s_log_var)      \n",
    "        zs = torch.cat([z, s], dim=1)\n",
    "\n",
    "        reconstructed_data_target = self.decoder(zs, batch_size)\n",
    "\n",
    "        z_mean_bg, z_log_var_bg = self.encoder_z(bg_image, batch_size)\n",
    "        z_bg = self.reparameterize(z_mean_bg, z_log_var_bg)\n",
    "        z_empty = torch.zeros_like(z_bg)\n",
    "        z_bg_0 = torch.cat([z_bg, z_empty], dim =1)\n",
    "        reconstructed_data_bg = self.decoder(z_bg_0, batch_size)\n",
    "        \n",
    "        return z_mean, z_log_var, s_mean, s_log_var, z_mean_bg, z_log_var_bg, reconstructed_data_target, reconstructed_data_bg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, I can't directly access the images to superimpose them. Therefore, I will have to get the images with the following function. Additionally, I have incorporated a greyscaler into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def load_databatch(data_folder, idx, img_size=32):\n",
    "    data_file = os.path.join(data_folder, 'train_data_batch_' + str(idx))\n",
    "    d = unpickle(data_file)\n",
    "    x = d['data']\n",
    "    y = d['labels']\n",
    "    mean_image = d['mean']\n",
    "\n",
    "    x = x / np.float32(255)\n",
    "    mean_image = mean_image / np.float32(255)\n",
    "\n",
    "    # Labels are indexed from 1, shift it so that indexes start at 0\n",
    "    y = [i - 1 for i in y]\n",
    "    data_size = x.shape[0]\n",
    "\n",
    "    x -= mean_image\n",
    "\n",
    "    img_size2 = img_size * img_size\n",
    "\n",
    "    x = np.dstack((x[:, :img_size2], x[:, img_size2:2 * img_size2], x[:, 2 * img_size2:]))\n",
    "    x = x.reshape((x.shape[0], img_size, img_size, 3)).transpose(0, 3, 1, 2)\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:data_size, :, :, :]\n",
    "    Y_train = y[0:data_size]\n",
    "    X_train_flip = X_train[:, :, :, ::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train, X_train_flip), axis=0)\n",
    "    Y_train = np.concatenate((Y_train, Y_train_flip), axis=0)\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train.astype(np.float32),\n",
    "        Y_train=np.array(Y_train, dtype=np.int32),\n",
    "        mean=mean_image.astype(np.float32)\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's superimpose them with each other!\n",
    "\n",
    "Firstly here is the way I loaded the mnist pictures as it was done before [here](./vae.ipynb). The data is loaded and resized to 32x32 to fit the Image-Net Data. \n",
    "I have once again split the data into a training and validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32, 32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=True, download=True, transform=transform\n",
    ")\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./input', train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I am unpickleing one batch and doing the calculations that were previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = load_databatch(\"./input/Imagenet32_train\", 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets split the channels into matrices to then greyscale them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape is now:  (256232, 3, 32, 32)\n",
      "The shape is now:  (256232, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "image_data = batch_1[\"X_train\"]\n",
    "\n",
    "# Get the number of images\n",
    "num_images = len(image_data)\n",
    "\n",
    "# Initialize an empty array to store the reshaped grayscale images\n",
    "grayscale_images = np.empty((num_images, 32, 32), dtype=np.float32)\n",
    "\n",
    "print(\"The shape is now: \",image_data.shape)\n",
    "\n",
    "# Reshape the images\n",
    "for i, image in enumerate(image_data):\n",
    "    # Reshape the image to match the color channel dimensions (3x32x32)\n",
    "    image = np.reshape(image, (3, 32, 32))\n",
    "\n",
    "    # Split the image into separate color channels\n",
    "    red_channel = image[0]\n",
    "    green_channel = image[1]\n",
    "    blue_channel = image[2]\n",
    "\n",
    "    # Combine the color channels weighted by their respective coefficients to form the grayscale image - these values are taken from recommedations from ChatGPT\n",
    "    grayscale_image = 0.2989 * red_channel + 0.5870 * green_channel + 0.1140 * blue_channel\n",
    "\n",
    "    # Normalize the grayscale image to the range [0, 1]\n",
    "    grayscale_image /= 255.0\n",
    "\n",
    "    # Store the grayscale image in the array\n",
    "    grayscale_images[i] = grayscale_image\n",
    "\n",
    "# Store the reshaped grayscale images in a new entry in the dictionary\n",
    "batch_1[\"X_train_gray\"] = grayscale_images\n",
    "print(\"The shape is now: \",grayscale_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seemed to have worked. Now, lets move on to overlaying them. The following still needs proper testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tchaase/miniconda3/envs/pytorch11.7/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlayed Images: 29898\n",
      "Not Overlayed Images: 226334\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "mnist_data = datasets.MNIST(root='./input', train=True, download=True, transform=transform)\n",
    "\n",
    "# Access the background images from the dictionary\n",
    "background_images = batch_1[\"X_train_gray\"]\n",
    "\n",
    "# Create a list to store the overlaid images\n",
    "target_images = []\n",
    "overlayed_count = 0\n",
    "\n",
    "# Create a list to store the background images (and load them)\n",
    "bg_images = []\n",
    "not_overlayed_count = 0\n",
    "\n",
    "# Assuming background_images and mnist_data are lists\n",
    "\n",
    "for i in range(len(background_images)):\n",
    "    background_image = background_images[i]\n",
    "    random_overlay = random.random() < 0.5  # Randomly choose whether to overlay MNIST image or not\n",
    "\n",
    "    if random_overlay and i < len(mnist_data):  # Overlay MNIST image\n",
    "        random_index = random.randint(0, len(mnist_data) - 1)  # Choose a random index from mnist_data\n",
    "        mnist_image, _ = mnist_data[random_index]\n",
    "\n",
    "        # Convert background image to tensor and move it to the CUDA device\n",
    "        background_tensor = torch.tensor(background_image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Resize the background image to match the MNIST image dimensions\n",
    "        resized_background_tensor = transforms.Resize((32, 32))(background_tensor)\n",
    "\n",
    "        # Resize MNIST image to match the background image dimensions\n",
    "        mnist_resized = transforms.Resize((32, 32))(mnist_image)\n",
    "\n",
    "        overlaid_tensor = resized_background_tensor + mnist_resized.to(device)\n",
    "\n",
    "        target_images.append(overlaid_tensor)  # Store information that the image was overlaid\n",
    "        overlayed_count += 1\n",
    "\n",
    "    else:\n",
    "        background_tensor = torch.tensor(background_image).unsqueeze(0).to(device)\n",
    "        bg_images.append(background_tensor)  # Store information that the image was not overlaid\n",
    "        not_overlayed_count += 1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# Print the counts of overlaid and non-overlaid images\n",
    "print(f\"Overlayed Images: {overlayed_count}\")\n",
    "print(f\"Not Overlayed Images: {not_overlayed_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I was a not so smart person before and started concatenating the images instead of using element wise addition, here is a constant reminder that I have now fixed this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of background images: 226334\n",
      "Number of overlaid images: 29898\n",
      "Background image dimensions: torch.Size([1, 32, 32])\n",
      "Overlaid image dimensions: torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "def check_image_dimensions(bg_images, target_images):\n",
    "    background_count = len(bg_images)\n",
    "    overlaid_count = len(target_images)\n",
    "\n",
    "    if background_count > 0:\n",
    "        background_dim = bg_images[0].shape\n",
    "    else:\n",
    "        background_dim = None\n",
    "\n",
    "    if overlaid_count > 0:\n",
    "        overlaid_dim = target_images[0].shape\n",
    "    else:\n",
    "        overlaid_dim = None\n",
    "\n",
    "    return background_count, overlaid_count, background_dim, overlaid_dim\n",
    "\n",
    "# Usage example:\n",
    "background_count, overlaid_count, background_dim, overlaid_dim = check_image_dimensions(bg_images, target_images)\n",
    "\n",
    "print(f\"Number of background images: {background_count}\")\n",
    "print(f\"Number of overlaid images: {overlaid_count}\")\n",
    "print(f\"Background image dimensions: {background_dim}\")\n",
    "print(f\"Overlaid image dimensions: {overlaid_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I check if anything is not a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_images:\n",
    "    if not torch.is_tensor(target) or target.shape != (1, 32, 32):\n",
    "        print(\"Non-tensor overlaid image found!\")\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for background in bg_images:\n",
    "    if not torch.is_tensor(background):\n",
    "        print(\"Non-tensor background found!\")\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the data-loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class OverlaidDataset(Dataset):\n",
    "    def __init__(self, target_images, bg_images):\n",
    "        self.target_images = target_images\n",
    "        self.bg_images = bg_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bg_images)  # Use the length of the background images list as the dataset length. I got more background images. \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_image = self.target_images[index % len(self.target_images)]  # Cycle through target images\n",
    "        bg_image = self.bg_images[index]\n",
    "\n",
    "        return target_image, bg_image\n",
    "\n",
    "# Create the overlaid dataset\n",
    "overlaid_dataset = OverlaidDataset(target_images, bg_images)\n",
    "\n",
    "# Create the dataloader\n",
    "batch_size = 64\n",
    "shuffle = True  # Shuffle the dataset to mix target and background images\n",
    "overlaid_dataloader = DataLoader(overlaid_dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking again that I still have the right data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_image is not a tensor of size 32x32. Instead it is:  torch.Size([30, 1, 32, 32])\n",
      "bg_image is not a tensor of size 32x32. Instead it is:  torch.Size([30, 1, 32, 32])\n",
      "There are  3536 batches with the target and  3536 without.\n"
     ]
    }
   ],
   "source": [
    "count_bg = 0\n",
    "count_target = 0\n",
    "\n",
    "for target_image, bg_image in overlaid_dataloader:\n",
    "    # Check if target_image is a tensor of size 32x32\n",
    "    if isinstance(target_image, torch.Tensor) and target_image.shape == (64, 1, 32, 32):\n",
    "        count_target += 1\n",
    "    else:\n",
    "        print(\"target_image is not a tensor of size 32x32. Instead it is: \", target_image.shape)\n",
    "\n",
    "    # Check if bg_image is a tensor of size 32x32\n",
    "    if isinstance(bg_image, torch.Tensor) and bg_image.shape == (64, 1, 32, 32):\n",
    "        count_bg += 1\n",
    "    else:\n",
    "        print(\"bg_image is not a tensor of size 32x32. Instead it is: \", bg_image.shape)\n",
    "\n",
    "print(\"There are \", count_target, \"batches with the target and \", count_bg, \"without.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am creating a training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:07<07:24, 501.66it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:32, 1465.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 7005.2512, Val Loss: 220762.7602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:07<07:29, 495.99it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:31, 1469.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 4221.6018, Val Loss: 119650.8346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:06<07:19, 507.48it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:32, 1458.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 3030.5017, Val Loss: 92807.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:06<07:20, 506.29it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:33, 1451.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 3450.9406, Val Loss: 85896.3352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:06<07:20, 505.68it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:33, 1449.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 3223.5853, Val Loss: 86772.6746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:07<07:21, 504.13it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:33, 1455.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 2902.8196, Val Loss: 80068.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:07<07:29, 495.45it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:34, 1444.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 2654.4165, Val Loss: 78110.3044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:07<07:32, 492.48it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:33, 1450.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 2521.5871, Val Loss: 72877.2094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:07<07:31, 493.45it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:32, 1462.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 2531.0235, Val Loss: 103164.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3537/226334 [00:07<07:21, 504.27it/s]\n",
      "  2%|▏         | 3537/226334 [00:02<02:32, 1464.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 2678.3102, Val Loss: 72219.9414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = cVAE(latent_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    train_loss_target, train_loss_bg = train(model, overlaid_dataloader, overlaid_dataset, device, optimizer, criterion)\n",
    "    \n",
    "    # Validate the model\n",
    "    val_loss, recon_images = validate(model, overlaid_dataloader, overlaid_dataset, device, criterion)\n",
    "    \n",
    "    # Print the losses\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss_target:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue is that I have a lot more background data, so I need to figure out a way to load more background images. thus have two seperate batch sizes. The data loader below also works, but I still need to figure out how I can properly adjust the amount of data I load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class OverlaidDataset(Dataset):\n",
    "    def __init__(self, target_images, bg_images):\n",
    "        self.target_images = target_images\n",
    "        self.bg_images = bg_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bg_images)  # Use the length of the background images list as the dataset length. I got more background images. \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_image = self.target_images[index % len(self.target_images)]  # Cycle through target images\n",
    "        bg_image = self.bg_images[index]\n",
    "\n",
    "        return target_image, bg_image\n",
    "\n",
    "# Create the overlaid dataset\n",
    "overlaid_dataset = OverlaidDataset(target_images, bg_images)\n",
    "\n",
    "# Create the dataloader\n",
    "batch_size = 64\n",
    "shuffle = True  # Shuffle the dataset to mix target and background images\n",
    "overlaid_dataloader = DataLoader(overlaid_dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
