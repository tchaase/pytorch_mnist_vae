{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for the MNIST-Data-Set\n",
    "\n",
    "[Tobias Haase](https://tchaase.github.io/)\n",
    "\n",
    "In the following I adapted [a blog on VAEs in particular for this data-set](https://debuggercafe.com/convolutional-variational-autoencoder-in-pytorch-on-mnist-dataset/). From this blog I took the basic code and then tried to understand whats going on from there. \n",
    "\n",
    "## Set Up\n",
    "Firstly I am loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tchaase/miniconda3/envs/pytorchml/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tchaase/miniconda3/envs/pytorchml/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/tchaase/miniconda3/envs/pytorchml/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import imageio "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I will define the functions I will use to train and validate the network. \n",
    "\n",
    "Firstly, the loss function is defined. The KL divergence is computed manually. There are three inputs: Firstly the reconstruction loss. \n",
    "Then there are the mean and the variance, which are related to the VAE's latent space.\n",
    "The loss is defined as the sum of the KL divergence and the reconstruction loss here, thus the sum is returned as the final loss.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the training function is defined. This takes the criterion refers to the loss function, which is the criterion that needs to be minimized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "        counter += 1  #There is a counter that is initialized at 0, which goes up for every batch?\n",
    "        data = data[0]\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        loss.backward()  # Using the loss, backpropagation occurs. Thus, all the tensors that will be connected to this, will be involved in this computation. \n",
    "        running_loss += loss.item() #This here defined for every step along the way, how high is the loss. \n",
    "        optimizer.step()  #That with a gradient will be updated in one step according to the documentation. \n",
    "    train_loss = running_loss / counter \n",
    "    return train_loss #The function returns only the training loss. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to validate that the training worked. Two things differentiate this from the training: Firstly, there is no backpropagation step here! The evaluation  does not impact the training. Then, images are saved. This is saved then according to the functions outlined in the utilis section. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
    "            counter += 1\n",
    "            data= data[0]\n",
    "            data = data.to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
    "                recon_images = reconstruction\n",
    "    val_loss = running_loss / counter\n",
    "    return val_loss, recon_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    " This section contains utility functions related to saving plots and images. They should not clutter up the training part as they are not relevant to training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image = transforms.ToPILImage() #conversion of a tensor to an image, this will later be used to generate .gif images!\n",
    "\n",
    "#The following function converts the PILutilityges to .gif files. The accepted data are numpy arrays!\n",
    "def image_to_vid(images):     \n",
    "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
    "    imageio.mimsave('../outputs/generated_images.gif', imgs)\n",
    "\n",
    "#This function is equal to the function above, but for outputs of the VAE\n",
    "def save_reconstructed_images(recon_images, epoch):\n",
    "    save_image(recon_images.cpu(), f\"../outputs/output{epoch}.jpg\")  #The save image function comes from torchvision.utilis!\n",
    "\n",
    "#Finally, here the training and validation losses are saved into a plot! This is done via matplotlib. \n",
    "def save_loss_plot(train_loss, valid_loss):\n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color='orange', label='train loss')\n",
    "    plt.plot(valid_loss, color='red', label='validataion loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('../outputs/loss.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Next I will define the parameters for the model. \n",
    "The kernel size is 4x4. This means that there is a 4 pixel wide and high kernel that is used for the convolutioin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4 # (4, 4) kernel\n",
    "init_channels = 8 # initial number of filters, first layers output. \n",
    "image_channels = 1 # MNIST images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step the full model is formulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Conv VAE\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    " \n",
    "        # encoder - Initially the VAE \n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_channels, out_channels=init_channels*2, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_channels*2, out_channels=init_channels*4, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_channels*4, out_channels=64, kernel_size=kernel_size, \n",
    "            stride=2, padding=0\n",
    "        )\n",
    "        # fully connected layers for learning representations <-- here there is the bottleneck\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "            # Using the 128 nodes as input for the bottleneck, the mean and variance of the latent channels are computed. \n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(128, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, 64)\n",
    "\n",
    "        # decoder \n",
    "        # Here there is the reverse ordere of the encoder, starting with 0 padding and moving on to a kernel with padding. \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=64, out_channels=init_channels*8, kernel_size=kernel_size, \n",
    "            stride=1, padding=0\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*8, out_channels=init_channels*4, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*4, out_channels=init_channels*2, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*2, out_channels=image_channels, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "\n",
    "    #The following is what allows us to backpropagate throught the model. One cannot do so through a random node, but we can sample from a random node and with the reparametrization trick \n",
    "    # still backpropagate through not random / deterministic nodes!\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size - this returns a vector filled with random entries. \n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        batch, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)  # This line I don't understand. \n",
    "        hidden = self.fc1(x)\n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.fc_mu(hidden)\n",
    "        log_var = self.fc_log_var(hidden)\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        z = self.fc2(z) # in this layer the reparametrization trick is applied. \n",
    "        z = z.view(-1, 64, 1, 1)\n",
    " \n",
    "        # decoding; \n",
    "        torch.manual_seed(0)\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        reconstruction = torch.sigmoid(self.dec4(x)) #Sigmoid for the final layer, this is fitting as we use cross entropy as to compute the loss? Linear functions would be associated with mean squared errors etc. \n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Firstly a few preparations are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.style.use('ggplot')\n",
    "# Firstly, the device is set to the GPU instead of the CPU for me, as I have cuda available. \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# initialize the model - here a convolutional VAE is used which will be computed on my GPU. What does this mean? It uses a convolution to analyze the images. \n",
    "model = ConvVAE().to(device)\n",
    "# set the learning parameters\n",
    "lr = 0.001  # I have no idea if this is large. \n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "# Trained 100 rounds, 64 samples each episode.  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)  # An ADAM optimizer is used. This means that the optimization occurs in one step, although I still need to read more on what this does exactly. \n",
    "criterion = nn.BCELoss(reduction='sum')  #BCE stands for Binary Cross Entropy, thus cross entropy is used as the criterion. \n",
    "# a list to save all the reconstructed images in PyTorch grid format\n",
    "grid_images = []  #An empty list is created. \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is transformed and loaded in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Transformation: \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "# The original data-set available from MNIST have a size of 28x28 pixels. Above they are transformed. \n",
    "\n",
    "# --- training set and train data loader\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='../input', train=True, download=True, transform=transform\n",
    ")\n",
    "# The data-set is loaded. It comes from the source of torchvision.data.set and is downloaded. \n",
    "trainloader = DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "# Here is the important set of actually loading the data. The batch sizes are also defined, thus it is defined how data will be sampled for later training?\n",
    "\n",
    "# --- validation set and validation data loader\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='../input', train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, every epoch 64 samples are run through the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall the test portion of the data-set has a length of %f 60000\n",
      "Overall the test portion of the data-set has a length of %f 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ../input\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Overall the test portion of the data-set has a length of %f\", len(trainset))\n",
    "print(\"Overall the test portion of the data-set has a length of %f\", len(testset))\n",
    "\n",
    "trainloader \n",
    "trainset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again empty lists are initialized for the training loss and the validation loss to allow for later plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the training function is defined. For every epoch in the range of epochs the following function will be computed. This means that for the first episode\n",
    "I will draw from the training set, compute the loss, backpropagate and rinse and repeat. \n",
    "The bread and butter of this are the train and validate functions. they require the input of the model, what data will be used to train, what data will be used to test. \n",
    "What is the device (for me GPU), then how will it be optimized (ADAM), in regards to the previously defined criterion of binary cross entropy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     train_epoch_loss \u001b[39m=\u001b[39m train(\n\u001b[1;32m      4\u001b[0m         model, trainloader, trainset, device, optimizer, criterion\n\u001b[1;32m      5\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(\n",
    "        model, trainloader, trainset, device, optimizer, criterion\n",
    "    )\n",
    "    valid_epoch_loss, recon_images = validate(\n",
    "        model, testloader, testset, device, criterion\n",
    "    )\n",
    "    # Next the values that result from this will be stored. \n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    # save the reconstructed images from the validation loop\n",
    "    save_reconstructed_images(recon_images, epoch+1)\n",
    "    # convert the reconstructed images to PyTorch image grid format, which can then be saved!\n",
    "    image_grid = make_grid(recon_images.detach().cpu())\n",
    "    grid_images.append(image_grid)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {valid_epoch_loss:.4f}\") #Up to 4 decimals will be printed, float format. \n",
    "\n",
    "# save the reconstructions as a .gif file\n",
    "image_to_vid(grid_images)\n",
    "# save the loss plots to disk\n",
    "save_loss_plot(train_loss, valid_loss)\n",
    "print('TRAINING COMPLETE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
